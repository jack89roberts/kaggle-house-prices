{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas for data loading, manipulation etc.\n",
    "import pandas as pd\n",
    "\n",
    "# numeric functions\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from math import ceil\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>Alley</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>Street</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>WD</td>\n",
       "      <td>0</td>\n",
       "      <td>Pave</td>\n",
       "      <td>8</td>\n",
       "      <td>856.0</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>WD</td>\n",
       "      <td>0</td>\n",
       "      <td>Pave</td>\n",
       "      <td>6</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>298</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>WD</td>\n",
       "      <td>0</td>\n",
       "      <td>Pave</td>\n",
       "      <td>6</td>\n",
       "      <td>920.0</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>WD</td>\n",
       "      <td>0</td>\n",
       "      <td>Pave</td>\n",
       "      <td>7</td>\n",
       "      <td>756.0</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>0</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>WD</td>\n",
       "      <td>0</td>\n",
       "      <td>Pave</td>\n",
       "      <td>9</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>192</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1stFlrSF  2ndFlrSF  3SsnPorch Alley  BedroomAbvGr BldgType BsmtCond  \\\n",
       "Id                                                                        \n",
       "1        856       854          0   NaN             3     1Fam       TA   \n",
       "2       1262         0          0   NaN             3     1Fam       TA   \n",
       "3        920       866          0   NaN             3     1Fam       TA   \n",
       "4        961       756          0   NaN             3     1Fam       Gd   \n",
       "5       1145      1053          0   NaN             4     1Fam       TA   \n",
       "\n",
       "   BsmtExposure  BsmtFinSF1  BsmtFinSF2  ...   SaleType ScreenPorch  Street  \\\n",
       "Id                                       ...                                  \n",
       "1            No       706.0         0.0  ...         WD           0    Pave   \n",
       "2            Gd       978.0         0.0  ...         WD           0    Pave   \n",
       "3            Mn       486.0         0.0  ...         WD           0    Pave   \n",
       "4            No       216.0         0.0  ...         WD           0    Pave   \n",
       "5            Av       655.0         0.0  ...         WD           0    Pave   \n",
       "\n",
       "    TotRmsAbvGrd TotalBsmtSF  Utilities WoodDeckSF YearBuilt YearRemodAdd  \\\n",
       "Id                                                                          \n",
       "1              8       856.0     AllPub          0      2003         2003   \n",
       "2              6      1262.0     AllPub        298      1976         1976   \n",
       "3              6       920.0     AllPub          0      2001         2002   \n",
       "4              7       756.0     AllPub          0      1915         1970   \n",
       "5              9      1145.0     AllPub        192      2000         2000   \n",
       "\n",
       "   YrSold  \n",
       "Id         \n",
       "1    2008  \n",
       "2    2007  \n",
       "3    2008  \n",
       "4    2006  \n",
       "5    2008  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df_train = pd.read_csv('train.csv',index_col='Id')\n",
    "df_test = pd.read_csv('test.csv',index_col='Id')\n",
    "\n",
    "# ids of full training dataset\n",
    "id_train = df_train.index    \n",
    "\n",
    "# ids of full test dataset\n",
    "id_test = df_test.index\n",
    "\n",
    "# combine train and test datas in to one dataframe\n",
    "df_all = pd.concat([df_train,df_test])\n",
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns with NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolQC          2909\n",
      "MiscFeature     2814\n",
      "Alley           2721\n",
      "Fence           2348\n",
      "SalePrice       1459\n",
      "FireplaceQu     1420\n",
      "LotFrontage      486\n",
      "GarageQual       159\n",
      "GarageCond       159\n",
      "GarageFinish     159\n",
      "GarageYrBlt      159\n",
      "GarageType       157\n",
      "BsmtExposure      82\n",
      "BsmtCond          82\n",
      "BsmtQual          81\n",
      "BsmtFinType2      80\n",
      "BsmtFinType1      79\n",
      "MasVnrType        24\n",
      "MasVnrArea        23\n",
      "MSZoning           4\n",
      "BsmtFullBath       2\n",
      "BsmtHalfBath       2\n",
      "Utilities          2\n",
      "Functional         2\n",
      "Electrical         1\n",
      "BsmtUnfSF          1\n",
      "Exterior1st        1\n",
      "Exterior2nd        1\n",
      "TotalBsmtSF        1\n",
      "GarageCars         1\n",
      "BsmtFinSF2         1\n",
      "BsmtFinSF1         1\n",
      "KitchenQual        1\n",
      "SaleType           1\n",
      "GarageArea         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# summary of columns with nan values\n",
    "#SalePrice nans: test data\n",
    "\n",
    "cols_with_na = df_all.isnull().sum()\n",
    "cols_with_na = cols_with_na[cols_with_na>0]\n",
    "print(cols_with_na.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaningful NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>None</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>RFn</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>No</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>Unf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>RFn</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>Unf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>RFn</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>Mn</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>Unf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Unf</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>No</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>Unf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>RFn</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>Av</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>Unf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PoolQC MiscFeature Alley Fence MasVnrType FireplaceQu GarageQual  \\\n",
       "Id                                                                    \n",
       "1    None        None  None  None    BrkFace        None         TA   \n",
       "2    None        None  None  None       None          TA         TA   \n",
       "3    None        None  None  None    BrkFace          TA         TA   \n",
       "4    None        None  None  None       None          Gd         TA   \n",
       "5    None        None  None  None    BrkFace          TA         TA   \n",
       "\n",
       "   GarageCond GarageFinish GarageType BsmtExposure BsmtCond BsmtQual  \\\n",
       "Id                                                                     \n",
       "1          TA          RFn     Attchd           No       TA       Gd   \n",
       "2          TA          RFn     Attchd           Gd       TA       Gd   \n",
       "3          TA          RFn     Attchd           Mn       TA       Gd   \n",
       "4          TA          Unf     Detchd           No       Gd       TA   \n",
       "5          TA          RFn     Attchd           Av       TA       Gd   \n",
       "\n",
       "   BsmtFinType1 BsmtFinType2  \n",
       "Id                            \n",
       "1           GLQ          Unf  \n",
       "2           ALQ          Unf  \n",
       "3           GLQ          Unf  \n",
       "4           ALQ          Unf  \n",
       "5           GLQ          Unf  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns where NaN values have meaning e.g. no pool etc.\n",
    "cols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n",
    "               'GarageQual','GarageCond','GarageFinish','GarageType',\n",
    "               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n",
    "\n",
    "# replace 'NaN' with 'None' in these columns\n",
    "for col in cols_fillna:\n",
    "    df_all[col].fillna('None',inplace=True)\n",
    "    \n",
    "#GarageYrBlt nans: no garage. Fill with property YearBuilt.\n",
    "#(more appropriate than 0, which would be ~2000 away from all other values)\n",
    "df_all.loc[df_all.GarageYrBlt.isnull(),'GarageYrBlt'] = df_all.loc[df_all.GarageYrBlt.isnull(),'YearBuilt']\n",
    "\n",
    "#No masonry veneer - fill area with 0\n",
    "df_all.MasVnrArea.fillna(0,inplace=True)\n",
    "\n",
    "#No basement - fill areas/counts with 0    \n",
    "df_all.BsmtFullBath.fillna(0,inplace=True)\n",
    "df_all.BsmtHalfBath.fillna(0,inplace=True)\n",
    "df_all.BsmtFinSF1.fillna(0,inplace=True)\n",
    "df_all.BsmtFinSF2.fillna(0,inplace=True)\n",
    "df_all.BsmtUnfSF.fillna(0,inplace=True)\n",
    "df_all.TotalBsmtSF.fillna(0,inplace=True)\n",
    "\n",
    "#No garage - fill areas/counts with 0\n",
    "df_all.GarageArea.fillna(0,inplace=True)\n",
    "df_all.GarageCars.fillna(0,inplace=True)\n",
    "\n",
    "df_all[cols_fillna].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LotFrontage NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to normalise a column of values to lie between 0 and 1\n",
    "def scale_minmax(col):\n",
    "    return (col-col.min())/(col.max()-col.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Intercept: 93.208483254\n",
      "----------------\n",
      "LotArea             71.475879\n",
      "RoofMatl_ClyTile    49.396968\n",
      "1stFlrSF            27.547832\n",
      "PoolArea            26.905146\n",
      "RoofStyle_Shed      25.874524\n",
      "GrLivArea           22.148975\n",
      "Foundation_Wood     20.075497\n",
      "LotShape_IR3        17.721471\n",
      "GarageArea          17.536239\n",
      "BldgType_2fmCon     16.123235\n",
      "dtype: float64\n",
      "----------------\n",
      "Utilities_AllPub      -11.610027\n",
      "BldgType_TwnhsE       -12.070870\n",
      "BsmtCond_Po           -12.151678\n",
      "Exterior1st_CemntBd   -14.553347\n",
      "PoolQC_Fa             -14.706527\n",
      "RoofMatl_WdShake      -15.579127\n",
      "BldgType_Twnhs        -17.924473\n",
      "RoofMatl_WdShngl      -18.222015\n",
      "GarageYrBlt           -21.591319\n",
      "LotConfig_CulDSac     -23.428742\n",
      "dtype: float64\n",
      "----------------\n",
      "R2: 0.688922936177\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "#LotFrontage\n",
    "# fill nan values using a linear regressor\n",
    "\n",
    "# convert categoricals to dummies, exclude SalePrice from model\n",
    "df_frontage = pd.get_dummies(df_all.drop('SalePrice',axis=1))\n",
    "\n",
    "# normalise columns to 0-1\n",
    "for col in df_frontage.drop('LotFrontage',axis=1).columns:\n",
    "    df_frontage[col] = scale_minmax(df_frontage[col])\n",
    "\n",
    "lf_train = df_frontage.dropna()\n",
    "lf_train_y = lf_train.LotFrontage\n",
    "lf_train_X = lf_train.drop('LotFrontage',axis=1)  \n",
    "\n",
    "# fit model\n",
    "lr = Ridge()\n",
    "lr.fit(lf_train_X, lf_train_y)\n",
    "\n",
    "# check model results\n",
    "lr_coefs = pd.Series(lr.coef_,index=lf_train_X.columns)\n",
    "\n",
    "print('----------------')\n",
    "print('Intercept:',lr.intercept_)\n",
    "print('----------------')\n",
    "print(lr_coefs.sort_values(ascending=False).head(10))\n",
    "print('----------------')\n",
    "print(lr_coefs.sort_values(ascending=False).tail(10))\n",
    "print('----------------')\n",
    "print('R2:',lr.score(lf_train_X,lf_train_y))\n",
    "print('----------------')\n",
    "\n",
    "lf_pred_y = lr.predict(lf_train_X)\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(lf_train_y,lf_pred_y,'.')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y_pred');\n",
    "plt.title('Prediction vs. True Value')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(lf_train_y, lf_train_y-lf_pred_y,'.')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y - y_pred');\n",
    "plt.title('Residual vs. True Value')\n",
    "\n",
    "\n",
    "# fill na values using model predictions\n",
    "nan_frontage = df_all.LotFrontage.isnull()\n",
    "X = df_frontage[nan_frontage].drop('LotFrontage',axis=1)\n",
    "y = lr.predict(X)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(y,bins=20)\n",
    "plt.title('Predicted Lot Frontages for NaN Values')\n",
    "\n",
    "# fill nan values\n",
    "df_all.loc[nan_frontage,'LotFrontage'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining Nan values\n",
    "cols_with_na = df_all.drop('SalePrice',axis=1).isnull().sum()\n",
    "cols_with_na = cols_with_na[cols_with_na>0]\n",
    "\n",
    "print(cols_with_na.sort_values(ascending=False))\n",
    "\n",
    "rows_with_na = df_all.drop('SalePrice',axis=1).isnull().sum(axis=1)\n",
    "rows_with_na = rows_with_na[rows_with_na>0]\n",
    "print(rows_with_na.sort_values(ascending=False))\n",
    "\n",
    "# fill remaining nans with mode in that column\n",
    "for col in cols_with_na.index:\n",
    "    df_all[col].fillna(df_all[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now no more NaN values apart from SalePrice in test data\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.SalePrice.describe())\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df_all.SalePrice.dropna() , fit=stats.norm);\n",
    "plt.subplot(1,2,2)\n",
    "_=stats.probplot(df_all.SalePrice.dropna(), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform SalePrice to improve normality\n",
    "sp = df_all.SalePrice\n",
    "df_all.SalePrice = np.log1p(sp)\n",
    "\n",
    "print(df_all.SalePrice.describe())\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(sp.dropna() , fit=stats.norm);\n",
    "plt.subplot(1,2,2)\n",
    "_=stats.probplot(sp.dropna(), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basement Finish Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate columns for area of each possible\n",
    "# basement finish type\n",
    "bsmt_fin_cols = ['BsmtGLQ','BsmtALQ','BsmtBLQ',\n",
    "                 'BsmtRec','BsmtLwQ','BsmtUnf']\n",
    "\n",
    "for col in bsmt_fin_cols:\n",
    "    # initialise as columns of zeros\n",
    "    df_all[col] = 0\n",
    "\n",
    "# assign old UnfSF column to the new one\n",
    "df_all['BsmtUnf'] = df_all['BsmtUnfSF']\n",
    "\n",
    "# fill remaining finish type columns\n",
    "for row in df_all.index:\n",
    "    fin1 = df_all.loc[row,'BsmtFinType1']\n",
    "    if (fin1!='None') and (fin1!='Unf'):\n",
    "        # add area (SF) to appropriate column\n",
    "        df_all.loc[row,'Bsmt'+fin1] += df_all.loc[row,'BsmtFinSF1']\n",
    "        \n",
    "    fin2 = df_all.loc[row,'BsmtFinType2']\n",
    "    if (fin2!='None') and (fin2!='Unf'):\n",
    "        df_all.loc[row,'Bsmt'+fin2] += df_all.loc[row,'BsmtFinSF2']\n",
    "\n",
    "# normalise new columns by TotalBsmtSF\n",
    "for col in bsmt_fin_cols:\n",
    "    df_all.loc[df_all['TotalBsmtSF']>0, col] = df_all.loc[df_all['TotalBsmtSF']>0,col] / df_all['TotalBsmtSF'] \n",
    "\n",
    "#plot to demonstrate the effect of the new feature\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.regplot(df_all.loc[(df_all['BsmtUnf']>0) & (df_all['BsmtUnf']<1), 'BsmtUnfSF'],\n",
    "            df_all.loc[(df_all['BsmtUnf']>0) & (df_all['BsmtUnf']<1), 'SalePrice'],\n",
    "            scatter_kws={'marker':'.','s':3,'alpha':0.5,'color':'r'},\n",
    "            line_kws={'color':'k'})\n",
    "\n",
    "plt.xlabel('BsmtUnf SF')\n",
    "plt.title('Before')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.regplot(df_all.loc[(df_all['BsmtUnf']>0) & (df_all['BsmtUnf']<1), 'BsmtUnf'],\n",
    "            df_all.loc[(df_all['BsmtUnf']>0) & (df_all['BsmtUnf']<1), 'SalePrice'],            \n",
    "            scatter_kws={'marker':'.','s':3,'alpha':0.5,'color':'b'},\n",
    "            line_kws={'color':'k'})\n",
    "\n",
    "plt.xlabel('BsmtUnf Fraction')\n",
    "plt.title('After')\n",
    "\n",
    "# remove initial BsmtFin columns\n",
    "df_all.drop(['BsmtFinType1','BsmtFinSF1','BsmtFinType2',\n",
    "             'BsmtFinSF2','BsmtUnfSF'], axis=1, inplace=True)\n",
    "\n",
    "df_all[bsmt_fin_cols].head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st and 2nd Floor Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['LowQualFin'] = df_all['LowQualFinSF']/df_all['GrLivArea']\n",
    "df_all['1stFlr'] = df_all['1stFlrSF']/df_all['GrLivArea']\n",
    "df_all['2ndFlr'] = df_all['2ndFlrSF']/df_all['GrLivArea']\n",
    "\n",
    "df_all.drop(['LowQualFinSF','1stFlrSF','2ndFlrSF'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of SalePrice in Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MSubClass should be treated as categorical\n",
    "df_all['MSSubClass'] = df_all['MSSubClass'].astype(object)\n",
    "\n",
    "# Extract categorical column names (have type object)\n",
    "cols_categ = df_all.dtypes[df_all.dtypes == object].index.tolist()\n",
    "\n",
    "# plot categorical variables\n",
    "fcols = 3\n",
    "frows = ceil(len(cols_categ)/fcols)\n",
    "plt.figure(figsize=(15,4*frows))\n",
    "\n",
    "for i,col in enumerate(cols_categ):\n",
    "    plt.subplot(frows,fcols,i+1)\n",
    "    sns.violinplot(df_all[col],df_all['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few bigger plots for features with two many categories to see above\n",
    "\n",
    "#Neighbourhood\n",
    "plt.figure(figsize=(25,5))\n",
    "sns.violinplot(x='Neighborhood',y='SalePrice',data=df_all)\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "#Exterior1st\n",
    "plt.figure(figsize=(25,5))\n",
    "sns.violinplot(x='Exterior1st',y='SalePrice',data=df_all)\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance of Categorical Features for SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# anova test to check significance of variation between groups\n",
    "def anova(group,value):\n",
    "    # select columns of interest, and remove any rows with nan values\n",
    "    data = df_all[[group,value]]\n",
    "    data = data[~(data[group].isnull() | data[value].isnull())]\n",
    "    \n",
    "    # stats across all data\n",
    "    tot_groups = data[group].nunique() # no. of groups\n",
    "    len_data = len(data) # total sample size of house (all groups)\n",
    "    mean_data = data[value].mean() # mean across all groups\n",
    "    df_betwn = tot_groups - 1 # degrees of freedom betwn grps\n",
    "    df_within = len_data - tot_groups # degrees of freedom within grps\n",
    "    \n",
    "    # per group stats\n",
    "    n_in_group = data.groupby(group)[value].count() # no. houses in group\n",
    "    mean_group = data.groupby(group)[value].mean() # mean value in this group\n",
    "    \n",
    "    # between-group variability\n",
    "    betwn_var = n_in_group*((mean_group - mean_data)**2)\n",
    "    betwn_var = float(betwn_var.sum())/df_betwn\n",
    "    \n",
    "    # within-group variability\n",
    "    within_var = 0\n",
    "    for grp in data[group].unique():\n",
    "        samples = data.loc[data[group]==grp, value]\n",
    "        within_var += ((samples-mean_group[grp])**2).sum()\n",
    "        \n",
    "    within_var = float(within_var)/df_within\n",
    "    \n",
    "    #F-test statistic\n",
    "    F = betwn_var/within_var\n",
    "    \n",
    "    # p-value\n",
    "    p = stats.f.sf(F, df_betwn, df_within)\n",
    "    \n",
    "    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check significance of categorical variables on SalePrice\n",
    "p_col = dict()\n",
    "\n",
    "for col in cols_categ:\n",
    "    p_col[col] = anova(col,'SalePrice')\n",
    "    \n",
    "pd.Series(p_col).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features with Meaningful Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert some categorical values to numeric scales\n",
    "\n",
    "#Excellent, Good, Typical, Fair, Poor, None: Convert to 0-5 scale\n",
    "cols_ExGd = ['ExterQual','ExterCond','BsmtQual','BsmtCond',\n",
    "             'HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n",
    "            'GarageCond','PoolQC']\n",
    "\n",
    "dict_ExGd = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n",
    "\n",
    "for col in cols_ExGd:\n",
    "    df_all[col].replace(dict_ExGd, inplace=True)\n",
    "\n",
    "display(df_all[cols_ExGd].head(5))    \n",
    "\n",
    "# Remaining columns\n",
    "df_all['BsmtExposure'].replace({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0}, inplace=True)\n",
    "\n",
    "df_all['CentralAir'].replace({'Y':1,'N':0}, inplace=True)\n",
    "\n",
    "df_all['Functional'].replace({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0}, inplace=True)\n",
    "\n",
    "df_all['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'None':0}, inplace=True)\n",
    "\n",
    "df_all['LotShape'].replace({'Reg':3,'IR1':2,'IR2':1,'IR3':0}, inplace=True)\n",
    "\n",
    "df_all['Utilities'].replace({'AllPub':3,'NoSewr':2,'NoSeWa':1,'ELO':0}, inplace=True)\n",
    "\n",
    "df_all['LandSlope'].replace({'Gtl':2,'Mod':1,'Sev':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Remaining Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining categorical columns\n",
    "\n",
    "# MSubClass should be treated as categorical\n",
    "df_all['MSSubClass'] = df_all['MSSubClass'].astype(object)\n",
    "\n",
    "cols_categ = df_all.dtypes[df_all.dtypes == object].index.tolist()\n",
    "\n",
    "print(len(cols_categ),'remaining categorical columns:\\n',cols_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace strings with mean SalePrice for that category value\n",
    "for col in cols_categ:\n",
    "    encoding = df_all.groupby(col).SalePrice.mean()\n",
    "    df_all[col].replace(encoding,inplace=True)\n",
    "\n",
    "    \n",
    "# one MSSubClass value in test set that is not\n",
    "# present in training set - replace with median\n",
    "df_all.MSSubClass.fillna(df_all.MSSubClass.median(),inplace=True)\n",
    "\n",
    "df_all[cols_categ].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fraction of zeros in each column\n",
    "frac_zeros = ((df_all==0).sum()/len(df_all))\n",
    "\n",
    "# no. unique values in each column\n",
    "n_unique = df_all.nunique()\n",
    "\n",
    "# difference between frac. zeros and expected\n",
    "# frac. zeros if values evenly distributed between\n",
    "# classes\n",
    "xs_zeros = frac_zeros - 1/n_unique\n",
    "\n",
    "# create dataframe and display which columns may be problematic\n",
    "zero_cols = pd.DataFrame({'frac_zeros':frac_zeros,'n_unique':n_unique,'xs_zeros':xs_zeros})\n",
    "zero_cols = zero_cols[zero_cols.frac_zeros>0]\n",
    "zero_cols.sort_values(by='xs_zeros',ascending=False,inplace=True)\n",
    "display(zero_cols[(zero_cols.xs_zeros>0)])\n",
    "\n",
    "#very few properties with Pool or 3SsnPorch\n",
    "#replace columns with binary indicator\n",
    "df_all['Pool'] = (df_all['PoolArea']>0).astype(int)\n",
    "df_all['3SsnPorch'] = (df_all['3SsnPorch']>0).astype(int)\n",
    "df_all.drop(['PoolQC','PoolArea','3SsnPorch'],axis=1,inplace=True)\n",
    "\n",
    "# 'half' bathrooms - add half value to 'full' bathrooms\n",
    "df_all['BsmtFullBath'] = df_all['BsmtFullBath'] + 0.5*df_all['BsmtHalfBath']\n",
    "df_all['FullBath'] = df_all['FullBath'] + 0.5*df_all['HalfBath']\n",
    "df_all.drop(['BsmtHalfBath','HalfBath'],axis=1,inplace=True)\n",
    "\n",
    "# create additional dummy variable for\n",
    "# continuous variables with a lot of zeros\n",
    "dummy_cols = ['LowQualFin','2ndFlr',\n",
    "              'MiscVal','ScreenPorch','WoodDeckSF','OpenPorchSF',\n",
    "              'EnclosedPorch','MasVnrArea','GarageArea','Fireplaces',             \n",
    "              'BsmtGLQ','BsmtALQ','BsmtBLQ','BsmtRec',\n",
    "              'BsmtLwQ','BsmtUnf','TotalBsmtSF']\n",
    "\n",
    "for col in dummy_cols:\n",
    "    df_all['Has'+col] = (df_all[col]>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM OF LOTS OF ZEROS -2ndFlrSF\n",
    "#####\n",
    "# mode only using 2ndflrsf\n",
    "#####\n",
    "tmp = scale_minmax(df_all[['SalePrice','2ndFlr','Has2ndFlr']].dropna())\n",
    "\n",
    "y = tmp.SalePrice\n",
    "X = tmp['2ndFlr']\n",
    "\n",
    "plt.plot(X,y,'.',label='data',alpha=0.5)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.values.reshape(-1, 1),y)\n",
    "lr_coefs = pd.Series(lr.coef_,index=['Has2ndFlr'])\n",
    "lr_intercept = lr.intercept_\n",
    "\n",
    "def regval(flr2ndSF):\n",
    "    return flr2ndSF*lr_coefs + lr_intercept\n",
    "\n",
    "plt.plot([0,1],[regval(0),regval(1)],'b',linewidth=3,label='2ndFlr only')\n",
    "\n",
    "#####\n",
    "# model using has2ndflr dummy variable\n",
    "#####\n",
    "tmp = scale_minmax(df_all[['SalePrice','2ndFlr','Has2ndFlr']].dropna())\n",
    "\n",
    "y = tmp.SalePrice\n",
    "\n",
    "X = tmp.drop('SalePrice',axis=1)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "lr_coefs = pd.Series(lr.coef_,index=X.columns)\n",
    "lr_intercept = lr.intercept_\n",
    "\n",
    "def regval(flr2ndSF,has2nd):\n",
    "    return flr2ndSF*lr_coefs['2ndFlr'] + has2nd*lr_coefs['Has2ndFlr'] + lr_intercept\n",
    "\n",
    "plt.plot([0,0.02,1],[regval(0,0),regval(0,1),regval(1,1)],'r',linewidth=3,label='with Has2ndFlr')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('2ndFlr')\n",
    "plt.ylabel('SalePrice');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Types of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any numeric column with less than 13 unique values to be\n",
    "# \"discrete\", other columns \"continuous\"\n",
    "# 13 chosen so months of the year are \"discrete\"\n",
    "col_nunique = df_all.nunique()\n",
    "\n",
    "cols_discrete = col_nunique[col_nunique<13].index.tolist()\n",
    "cols_continuous = col_nunique[col_nunique>=13].index.tolist()\n",
    "\n",
    "print(len(cols_continuous),'continuous columns, and',\n",
    "      len(cols_discrete),'discrete columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Continuous Variables and Effect on SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot numeric values\n",
    "\n",
    "fcols = 2\n",
    "frows = len(cols_continuous)\n",
    "plt.figure(figsize=(5*fcols,4*frows))\n",
    "\n",
    "i=0\n",
    "for col in cols_continuous:\n",
    "    i+=1\n",
    "    ax=plt.subplot(frows,fcols,i)\n",
    "    sns.regplot(x=col, y='SalePrice', data=df_all, ax=ax, \n",
    "                scatter_kws={'marker':'.','s':3,'alpha':0.3},\n",
    "                line_kws={'color':'k'});\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('SalePrice')\n",
    "    \n",
    "    i+=1\n",
    "    ax=plt.subplot(frows,fcols,i)\n",
    "    sns.distplot(df_all[col].dropna() , fit=stats.norm)\n",
    "    plt.xlabel(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Discrete Variables and Effect on SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numeric columns with few values\n",
    "fcols = 3\n",
    "frows = ceil(len(cols_discrete)/fcols)\n",
    "plt.figure(figsize=(15,4*frows))\n",
    "\n",
    "for i,col in enumerate(cols_discrete):\n",
    "    plt.subplot(frows,fcols,i+1)\n",
    "    sns.violinplot(df_all[col],df_all['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance of Discrete Numeric Features for SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_col = dict()\n",
    "\n",
    "for col in cols_discrete:\n",
    "    p_col[col] = anova(col,'SalePrice')\n",
    "    \n",
    "pd.Series(p_col).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Between Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation between numeric variables\n",
    "df_corr = df_all.loc[id_train].corr(method='spearman').abs()\n",
    "\n",
    "# order columns and rows by correlation with SalePrice\n",
    "df_corr = df_corr.sort_values('SalePrice',axis=0,ascending=False).sort_values('SalePrice',axis=1,ascending=False)\n",
    "\n",
    "print(df_corr.SalePrice.head(20))\n",
    "print('-----------------')\n",
    "print(df_corr.SalePrice.tail(10))\n",
    "\n",
    "ax=plt.figure(figsize=(25,20)).gca()\n",
    "sns.heatmap(df_corr,ax=ax,square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity\n",
    "\n",
    "Many variables strongly correlated, e.g. GarageCars and GarageArea (below).\n",
    "\n",
    "Kept all features, but feature selection/pca may help some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='GarageCars',y='GarageArea',data=df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise numeric columns\n",
    "scale_cols = [col for col in df_all.columns if col!='SalePrice']\n",
    "\n",
    "df_all[scale_cols] = df_all[scale_cols].apply(scale_minmax,axis=0)\n",
    "\n",
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Features are Candidates to be Transformed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check effect of Box-Cox transforms on distributions of continuous variables\n",
    "fcols = 6\n",
    "frows = len(cols_continuous)-1\n",
    "plt.figure(figsize=(4*fcols,4*frows))\n",
    "i=0\n",
    "\n",
    "for var in cols_continuous:\n",
    "    if var!='SalePrice':\n",
    "        dat = df_all[[var, 'SalePrice']].dropna()\n",
    "        \n",
    "        i+=1\n",
    "        plt.subplot(frows,fcols,i)\n",
    "        sns.distplot(dat[var] , fit=stats.norm);\n",
    "        plt.title(var+' Original')\n",
    "        plt.xlabel('')\n",
    "        \n",
    "        i+=1\n",
    "        plt.subplot(frows,fcols,i)\n",
    "        _=stats.probplot(dat[var], plot=plt)\n",
    "        plt.title('skew='+'{:.4f}'.format(stats.skew(dat[var])))\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        i+=1\n",
    "        plt.subplot(frows,fcols,i)\n",
    "        plt.plot(dat[var], dat['SalePrice'],'.',alpha=0.5)\n",
    "        plt.title('corr='+'{:.2f}'.format(np.corrcoef(dat[var], dat['SalePrice'])[0][1]))\n",
    " \n",
    "        i+=1\n",
    "        plt.subplot(frows,fcols,i)\n",
    "        trans_var, lambda_var = stats.boxcox(dat[var].dropna()+1)\n",
    "        trans_var = scale_minmax(trans_var)      \n",
    "        sns.distplot(trans_var , fit=stats.norm);\n",
    "        plt.title(var+' Tramsformed')\n",
    "        plt.xlabel('')\n",
    "        \n",
    "        i+=1\n",
    "        plt.subplot(frows,fcols,i)\n",
    "        _=stats.probplot(trans_var, plot=plt)\n",
    "        plt.title('skew='+'{:.4f}'.format(stats.skew(trans_var)))\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        i+=1\n",
    "        plt.subplot(frows,fcols,i)\n",
    "        plt.plot(trans_var, dat['SalePrice'],'.',alpha=0.5)\n",
    "        plt.title('corr='+'{:.2f}'.format(np.corrcoef(trans_var,dat['SalePrice'])[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-Cox Transform Suitable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables not suitable for box-cox transformation based on above (usually due to excessive zeros)\n",
    "# NB - have tried BoxCox on SalePrice, but no improvement or slightly worse\n",
    "# than log transform. Kept log transform to make it simpler to restore SalePrice predictions.\n",
    "cols_notransform = ['1stFlr','2ndFlr','3SsnPorch','EnclosedPorch',\n",
    "                    'LowQualFin','MasVnrArea','MiscVal','PoolArea',\n",
    "                    'ScreenPorch','OpenPorchSF','WoodDeckSF','SalePrice',\n",
    "                    'BsmtGLQ','BsmtALQ','BsmtBLQ','BsmtRec','BsmtLwQ','BsmtUnf']\n",
    "\n",
    "cols_transform = [col for col in cols_continuous if col not in cols_notransform]\n",
    "\n",
    "#transform remaining variables\n",
    "print('Transforming',len(cols_transform),'columns:',cols_transform)\n",
    "\n",
    "for col in cols_transform: \n",
    "    # transform column\n",
    "    df_all.loc[:,col], _ = stats.boxcox(df_all.loc[:,col]+1)\n",
    "    \n",
    "    # renormalise column\n",
    "    df_all.loc[:,col] = scale_minmax(df_all.loc[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features, encode categoricals, create dataframe for model fitting\n",
    "\n",
    "# select which features to use (all for now)\n",
    "model_cols = df_all.columns\n",
    "\n",
    "# encode categoricals\n",
    "df_model = df_all[model_cols]\n",
    "        \n",
    "display(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metric for evaluation\n",
    "def rmse(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    sum_sq = sum(diff**2)    \n",
    "    n = len(y_pred)   \n",
    "    \n",
    "    return np.sqrt(sum_sq/n)\n",
    "\n",
    "# scorer to be used in sklearn model fitting\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to detect outliers based on the predictions of a model\n",
    "def remove_outliers(model, X, y, sigma=3, drop=True, verbose=False):\n",
    "\n",
    "    # predict y values using model\n",
    "    try:\n",
    "        y_pred = pd.Series(model.predict(X), index=y.index)\n",
    "    # if predicting fails, try fitting the model first\n",
    "    except:\n",
    "        model.fit(X,y)\n",
    "        y_pred = pd.Series(model.predict(X), index=y.index)\n",
    "        \n",
    "    # calculate residuals between the model prediction and true y values\n",
    "    resid = y - y_pred\n",
    "    mean_resid = resid.mean()\n",
    "    std_resid = resid.std()\n",
    "\n",
    "    # calculate z statistic, define outliers to be where |z|>sigma\n",
    "    z = (resid - mean_resid)/std_resid    \n",
    "    outliers = z[abs(z)>3].index\n",
    "    \n",
    "    # print and plot the results if asked\n",
    "    if verbose:\n",
    "        print('R2=',model.score(X,y))\n",
    "        print('rmse=',rmse(y, y_pred))\n",
    "        print('---------------------------------------')\n",
    "\n",
    "        print('mean of residuals:',mean_resid)\n",
    "        print('std of residuals:',std_resid)\n",
    "        print('---------------------------------------')\n",
    "        \n",
    "        print(len(outliers),'outliers:')\n",
    "        print(outliers.tolist())\n",
    "        \n",
    "        plt.figure(figsize=(15,5))\n",
    "        ax_131 = plt.subplot(1,3,1)\n",
    "        plt.plot(y,y_pred,'.')\n",
    "        plt.plot(y.loc[outliers],y_pred.loc[outliers],'ro')\n",
    "        plt.legend(['Accepted','Outlier'])\n",
    "        plt.xlabel('y')\n",
    "        plt.ylabel('y_pred');\n",
    "\n",
    "        ax_132=plt.subplot(1,3,2)\n",
    "        plt.plot(y,y-y_pred,'.')\n",
    "        plt.plot(y.loc[outliers],y.loc[outliers]-y_pred.loc[outliers],'ro')\n",
    "        plt.legend(['Accepted','Outlier'])\n",
    "        plt.xlabel('y')\n",
    "        plt.ylabel('y - y_pred');\n",
    "\n",
    "        ax_133=plt.subplot(1,3,3)\n",
    "        z.plot.hist(bins=50,ax=ax_133)\n",
    "        z.loc[outliers].plot.hist(color='r',bins=50,ax=ax_133)\n",
    "        plt.legend(['Accepted','Outlier'])\n",
    "        plt.xlabel('z')\n",
    "\n",
    "    # remove outliers from data if requested\n",
    "    if drop:\n",
    "        X = X.drop(outliers)\n",
    "        y = y.drop(outliers)\n",
    "\n",
    "    return X,y,outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to get training samples\n",
    "def get_training_data():\n",
    "    # extract training samples\n",
    "    df_train = df_model.loc[id_train]\n",
    "    \n",
    "    # split SalePrice and features\n",
    "    y = df_train.SalePrice\n",
    "    X = df_train.drop('SalePrice',axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# extract test data (without SalePrice)\n",
    "def get_test_data():\n",
    "    return df_model.loc[id_test].drop('SalePrice',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data\n",
    "X, y = get_training_data()\n",
    "\n",
    "# find and remove outliers using a Ridge model\n",
    "X,y,outliers = remove_outliers(Ridge(), X, y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model - LinearRegression with Regularisation (Ridge)\n",
    "\n",
    "0.1207 on leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# basic linear model\n",
    "lr = Ridge()\n",
    "lr.fit(X,y)\n",
    "lr_coefs = pd.Series(lr.coef_,index=X.columns)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('Top 20 contributers to increased price:')\n",
    "print('---------------------------------------')\n",
    "print(lr_coefs.sort_values(ascending=False).head(20))\n",
    "print('---------------------------------------')\n",
    "print('Top 20 contributers to decreased price:')\n",
    "print('---------------------------------------')\n",
    "print(lr_coefs.sort_values(ascending=True).head(20))\n",
    "print('---------------------------------------')\n",
    "print('Intercept: ',lr.intercept_)\n",
    "print('---------------------------------------')\n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "rms_pred = rmse(y, y_pred)\n",
    "\n",
    "print('R2=',lr.score(X,y))\n",
    "print('rmse=',rms_pred)\n",
    "rkfold = RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "print('cross_val',\n",
    "      abs(np.mean(cross_val_score(lr, X, y, \n",
    "                                  scoring=rmse_scorer, \n",
    "                                  cv=rkfold))))\n",
    "print('---------------------------------------')\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(y,y_pred,'.')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y_pred');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(y,y-y_pred,'.')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y - y_pred');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, param_grid=[], X=[], y=[], \n",
    "                splits=5, repeats=5):\n",
    "\n",
    "    # get unmodified training data, unless data to use already specified\n",
    "    if len(y)==0:\n",
    "        X,y = get_training_data()\n",
    "    \n",
    "    # create cross-validation method\n",
    "    rkfold = RepeatedKFold(n_splits=splits, n_repeats=repeats)\n",
    "    \n",
    "    # perform a grid search if param_grid given\n",
    "    if len(param_grid)>0:\n",
    "        # setup grid search parameters\n",
    "        gsearch = GridSearchCV(model, param_grid, cv=rkfold,\n",
    "                               scoring=rmse_scorer,\n",
    "                               verbose=1, return_train_score=True)\n",
    "\n",
    "        # search the grid\n",
    "        gsearch.fit(X,y)\n",
    "\n",
    "        # extract results and best model from the grid\n",
    "        grid_results = pd.DataFrame(gsearch.cv_results_)\n",
    "        model = gsearch.best_estimator_\n",
    "    else:\n",
    "        grid_results = []\n",
    "        \n",
    "    # re-fit the model with the same parameters, but excluding outliers\n",
    "    X,y,outliers = remove_outliers(model, X, y)\n",
    "    model.fit(X,y)\n",
    "\n",
    "    cv_score = cross_val_score(model, X, y, scoring=rmse_scorer, cv=rkfold)\n",
    "    cv_mean = abs(np.mean(cv_score))\n",
    "    cv_std = np.std(cv_score)\n",
    "    cv_score = pd.Series({'mean':cv_mean,'std':cv_std})\n",
    "\n",
    "    # print stats on model performance         \n",
    "    print('----------------------')\n",
    "    print(model)\n",
    "    print('----------------------')\n",
    "    print('score=',model.score(X,y))\n",
    "    print('rmse=',rmse(y, model.predict(X)))\n",
    "    print('cross_val: mean=',cv_mean,', std=',cv_std)\n",
    "    \n",
    "    # residual plots\n",
    "    y_pred = pd.Series(model.predict(X),index=y.index)\n",
    "    resid = y - y_pred\n",
    "    mean_resid = resid.mean()\n",
    "    std_resid = resid.std()\n",
    "    z = (resid - mean_resid)/std_resid    \n",
    "    n_outliers = sum(abs(z)>3)\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    ax_131 = plt.subplot(1,3,1)\n",
    "    plt.plot(y,y_pred,'.')\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('y_pred');\n",
    "    plt.title('corr = {:.3f}'.format(np.corrcoef(y,y_pred)[0][1]))\n",
    "    ax_132=plt.subplot(1,3,2)\n",
    "    plt.plot(y,y-y_pred,'.')\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('y - y_pred');\n",
    "    plt.title('std resid = {:.3f}'.format(std_resid))\n",
    "    \n",
    "    ax_133=plt.subplot(1,3,3)\n",
    "    z.plot.hist(bins=50,ax=ax_133)\n",
    "    plt.xlabel('z')\n",
    "    plt.title('{:.0f} samples with z>3'.format(n_outliers))\n",
    "\n",
    "    return model, cv_score, grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to store optimal models\n",
    "opt_models = dict()\n",
    "score_models = pd.DataFrame(columns=['mean','std'])\n",
    "\n",
    "# no. k-fold splits\n",
    "splits=5\n",
    "# no. k-fold iterations\n",
    "repeats=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = 'Ridge'\n",
    "\n",
    "opt_models[model] = Ridge()\n",
    "alph_range = np.arange(0.25,6,0.25)\n",
    "param_grid = {'alpha': alph_range}\n",
    "\n",
    "opt_models[model],cv_score,grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=repeats)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(alph_range, abs(grid_results['mean_test_score']),\n",
    "             abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Lasso'\n",
    "\n",
    "opt_models[model] = Lasso()\n",
    "alph_range = np.arange(1e-4,1e-3,4e-5)\n",
    "param_grid = {'alpha': alph_range}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=repeats)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(alph_range, abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model ='ElasticNet'\n",
    "opt_models[model] = ElasticNet()\n",
    "\n",
    "param_grid = {'alpha': np.arange(0.1,1.1,0.1),\n",
    "              'l1_ratio': np.arange(0.1,1.0,0.1)}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=repeats)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='LinearSVR'\n",
    "opt_models[model] = LinearSVR()\n",
    "\n",
    "crange = np.arange(0.1,1.0,0.1)\n",
    "param_grid = {'C':crange}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=repeats)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(crange, abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ='SVR'\n",
    "opt_models[model] = SVR()\n",
    "\n",
    "param_grid = {'C':np.arange(1,21,2),\n",
    "              'kernel':['poly','rbf','sigmoid']}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=repeats)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'KNeighbors'\n",
    "opt_models[model] = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {'n_neighbors':np.arange(3,11,1)}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=repeats)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(np.arange(3,11,1), abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'RandomForest'\n",
    "opt_models[model] = RandomForestRegressor()\n",
    "\n",
    "param_grid = {'n_estimators':[100,150,200],\n",
    "              'max_features':[25,50,75],\n",
    "              'min_samples_split':[2,4,6]}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=5, repeats=1)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GradientBoosting'\n",
    "opt_models[model] = GradientBoostingRegressor()\n",
    "\n",
    "param_grid = {'n_estimators':[100,175,250],\n",
    "              'max_depth':[2,3,4],\n",
    "              'min_samples_split':[5,6,7]}\n",
    "\n",
    "opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=splits, repeats=1)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'XGB'\n",
    "opt_models[model] = XGBRegressor()\n",
    "\n",
    "param_grid = {'n_estimators':[100,200,300,400,500],\n",
    "              'max_depth':[1,2,3],\n",
    "             }\n",
    "\n",
    "opt_models[model], cv_score,grid_results = train_model(opt_models[model], param_grid=param_grid, \n",
    "                                              splits=5, repeats=1)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA then Ridge with 2nd Order Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'PolyPCA'\n",
    "\n",
    "X,y=get_training_data()\n",
    "\n",
    "# get 2nd order terms\n",
    "poly = PolynomialFeatures()\n",
    "X = poly.fit_transform(X,y)\n",
    "\n",
    "# reduce dimensionality using PCA\n",
    "# n_components = len(X) by default\n",
    "# (as no. polynomial features >> len(x))\n",
    "pca = PCA()\n",
    "X = pca.fit_transform(X,y)\n",
    "X = pd.DataFrame(X,index=y.index)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('no. components')\n",
    "plt.ylabel('cumulative explained variance ratio')\n",
    "plt.title('PCA result')\n",
    "\n",
    "alph_range = np.arange(5,100,5)\n",
    "param_grid = {'alpha':alph_range}\n",
    "ridge, cv_score, grid_results = train_model(Ridge(),X=X,y=y,param_grid=param_grid)\n",
    "\n",
    "cv_score.name = model\n",
    "score_models = score_models.append(cv_score)\n",
    "\n",
    "# combine results in to a pipeline\n",
    "opt_models[model] = Pipeline([('Poly',poly),('PCA',pca),('Ridge',ridge)])\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(alph_range, abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_training_data()\n",
    "\n",
    "Y_models = pd.DataFrame(index=y.index)    \n",
    "\n",
    "for key, model in opt_models.items():\n",
    "    Y_models[key] = model.predict(X)\n",
    "\n",
    "corr_models = pd.DataFrame(Y_models).corr()\n",
    "\n",
    "score_models = score_models.sort_values(by='mean')\n",
    "\n",
    "display(score_models)\n",
    "score_models['mean'].plot.barh()\n",
    "plt.xlabel('score')\n",
    "\n",
    "plt.figure()\n",
    "display(corr_models)\n",
    "sns.heatmap(corr_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict SalePrice for the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test(model):\n",
    "    # get test data\n",
    "    X_test = get_test_data()\n",
    "\n",
    "    # predict SalePrices\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    #revert log transformation\n",
    "    y_test = np.exp(y_test)-1\n",
    "\n",
    "    y_test = pd.DataFrame(y_test,index=X_test.index)\n",
    "    y_test.columns = ['SalePrice']\n",
    "    \n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = predict_test(opt_models['Lasso'])\n",
    "\n",
    "y_test.plot.hist(bins=20)\n",
    "\n",
    "# make submission file\n",
    "y_test.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model\n",
    "\n",
    "Not found an approach that gives a score improvement over just Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to try in an ensemble model:\n",
    "# 1/score^2 normalised to sum to 1\n",
    "def model_weights(models):\n",
    "    weights = 1/(score_models.loc[models,'mean']**2)\n",
    "    weights = weights/sum(weights)\n",
    "    return weights\n",
    "\n",
    "# predict a SalePrice using multiple models\n",
    "def predict_ensemble(models):\n",
    "    # get test data\n",
    "    X_test = get_test_data()\n",
    "   \n",
    "    # predict SalePrice for each model\n",
    "    Y_models = pd.DataFrame(index=X_test.index)    \n",
    "    for model in models:\n",
    "        Y_models[model] = opt_models[model].predict(X_test)\n",
    "\n",
    "    # get weights for each model\n",
    "    weights = model_weights(models)\n",
    "    \n",
    "    # calculate final prediction\n",
    "    Y_models = Y_models*weights\n",
    "    y_test = Y_models.sum(axis=1)\n",
    "\n",
    "    #revert log transformation\n",
    "    y_test = np.exp(y_test)-1\n",
    "    \n",
    "    # convert to dataframe suitable for creating submission file\n",
    "    y_test = pd.DataFrame(y_test,index=X_test.index)\n",
    "    y_test.columns = ['SalePrice']\n",
    "    \n",
    "    return y_test\n",
    "\n",
    "#example with top 5 models\n",
    "print(model_weights(score_models.iloc[:5].index))\n",
    "y_test = predict_ensemble(score_models.iloc[:5].index)\n",
    "y_test.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Think about:\n",
    "# Outlier removal - may be harmful some cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs = pd.Series(opt_models['Lasso'].coef_,index=X.columns).sort_values()\n",
    "plt.figure(figsize=(10,20))\n",
    "lasso_coefs[lasso_coefs!=0].plot.barh()\n",
    "print('------------------------')\n",
    "print(sum(lasso_coefs==0),'zero coefficients:')\n",
    "print('------------------------')\n",
    "print(lasso_coefs[lasso_coefs==0].index.sort_values().tolist())\n",
    "print('------------------------')\n",
    "print(sum(lasso_coefs!=0),'non-zero coefficients:')\n",
    "print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
